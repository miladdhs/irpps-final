#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Content
Ù‡Ø± ÙØ§ÛŒÙ„ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ù…Ø­ØªÙˆØ§ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÛŒâ€ŒØ´ÙˆØ¯
"""

import json
import re
import os
from datetime import datetime
from pathlib import Path
import unicodedata
import html

# Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡ Content
CONTENT_DIR = Path("frontend/public/Content")
OUTPUT_FILE = "content_extracted.json"

def persian_to_slug(text):
    """ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø§Ø³Ù„Ø§Ú¯ URL-friendly"""
    if not text:
        return ""
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
    persian_digits = 'Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹'
    english_digits = '0123456789'
    for i, p in enumerate(persian_digits):
        text = text.replace(p, english_digits[i])
    
    # Ø¬Ø¯ÙˆÙ„ ØªØ¨Ø¯ÛŒÙ„ Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ù„Ø§ØªÛŒÙ†
    persian_to_latin = {
        'Ø¢': 'a', 'Ø§': 'a', 'Ø¨': 'b', 'Ù¾': 'p', 'Øª': 't', 'Ø«': 's',
        'Ø¬': 'j', 'Ú†': 'ch', 'Ø­': 'h', 'Ø®': 'kh', 'Ø¯': 'd', 'Ø°': 'z',
        'Ø±': 'r', 'Ø²': 'z', 'Ú˜': 'zh', 'Ø³': 's', 'Ø´': 'sh', 'Øµ': 's',
        'Ø¶': 'z', 'Ø·': 't', 'Ø¸': 'z', 'Ø¹': 'a', 'Øº': 'gh', 'Ù': 'f',
        'Ù‚': 'gh', 'Ú©': 'k', 'Ú¯': 'g', 'Ù„': 'l', 'Ù…': 'm', 'Ù†': 'n',
        'Ùˆ': 'v', 'Ù‡': 'h', 'ÛŒ': 'y', 'Ø¦': 'y', 'ÙŠ': 'y',
        ' ': '-', '_': '-', '.': '', ',': '', 'ØŒ': '', 'Ø›': '', ':': '',
        'ØŸ': '', '!': '', '(': '', ')': '', '[': '', ']': '', '{': '', '}': '',
        '/': '-', '\\': '-', '|': '-', '"': '', "'": '', 'Â«': '', 'Â»': '',
    }
    
    result = []
    for char in text:
        if char in persian_to_latin:
            result.append(persian_to_latin[char])
        elif char.isalnum():
            result.append(char.lower())
        elif char in ['-', '_']:
            result.append('-')
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø±Ø´ØªÙ‡ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
    slug = ''.join(result)
    slug = re.sub(r'[^a-z0-9-]', '', slug.lower())
    slug = re.sub(r'-+', '-', slug)
    slug = slug.strip('-')
    
    # Ø§Ú¯Ø± Ø®Ø§Ù„ÛŒ Ø´Ø¯ØŒ ÛŒÚ© Ø§Ø³Ù„Ø§Ú¯ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø³Ø§Ø²
    if not slug:
        slug = 'item-' + str(abs(hash(text)) % 10000)
    
    return slug

def extract_short_content(content, max_length=500):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®Ù„Ø§ØµÙ‡ Ø§Ø² Ù…Ø­ØªÙˆØ§"""
    if not content:
        return None
    
    # Ø­Ø°Ù HTML tags Ùˆ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
    content = re.sub(r'<[^>]+>', '', content)
    content = re.sub(r'https?://[^\s]+', '', content)
    content = re.sub(r'\s+', ' ', content)
    content = content.strip()
    
    # Ø­Ø°Ù Ø®Ø·ÙˆØ· Ø®Ø§Ù„ÛŒ Ùˆ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
    lines = [line.strip() for line in content.split('\n') if line.strip()]
    content = ' '.join(lines)
    
    if len(content) <= max_length:
        return content
    
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù†Ù‚Ø·Ù‡ Ù¾Ø§ÛŒØ§Ù† Ù…Ù†Ø§Ø³Ø¨
    truncated = content[:max_length]
    last_period = truncated.rfind('.')
    last_space = truncated.rfind(' ')
    
    if last_period > max_length * 0.7:
        return truncated[:last_period + 1]
    elif last_space > max_length * 0.7:
        return truncated[:last_space] + '...'
    else:
        return truncated + '...'

def extract_title_from_filename(filename):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„"""
    # Ø­Ø°Ù Ù¾Ø³ÙˆÙ†Ø¯
    name = Path(filename).stem
    # Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§
    name = re.sub(r'\d{4}-\d{2}-\d{2}', '', name)
    name = re.sub(r'\(\d+\)', '', name)
    name = re.sub(r'^\d+', '', name)
    name = name.strip(' -_')
    
    # Ø§Ú¯Ø± Ù†Ø§Ù… ÙØ§ÛŒÙ„ ÙØ§Ø±Ø³ÛŒ Ø§Ø³ØªØŒ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
    if any('\u0600' <= char <= '\u06FF' for char in name):
        return name.replace('_', ' ').replace('-', ' ')
    
    return None

def process_html_file(file_path):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ HTML"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            html_content = f.read()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù†
        title_match = re.search(r'<title>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
        title = title_match.group(1).strip() if title_match else "ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§ÛŒØ±Ø§Ù†"
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ ÙØ§Ø±Ø³ÛŒ
        persian_section = re.search(r'<div class="card persian">(.*?)</div>', html_content, re.DOTALL)
        if persian_section:
            content = persian_section.group(1)
            # Ø­Ø°Ù HTML tags
            content = re.sub(r'<[^>]+>', '', content)
            content = html.unescape(content)
            content = re.sub(r'\s+', ' ', content).strip()
            
            # ØªØ¹ÛŒÛŒÙ† Ù†ÙˆØ¹ Ù…Ø­ØªÙˆØ§
            if "ØªØ§Ø±ÛŒØ®Ú†Ù‡" in title or "history" in title.lower():
                return {
                    "type": "news",
                    "title": title,
                    "slug": persian_to_slug(title),
                    "content": content,
                    "short_content": extract_short_content(content),
                    "category": "ØªØ§Ø±ÛŒØ®Ú†Ù‡",
                    "tags": "ØªØ§Ø±ÛŒØ®Ú†Ù‡,Ø§Ù†Ø¬Ù…Ù†,Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù†",
                    "source": "Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§ÛŒØ±Ø§Ù†",
                    "image": None
                }
    except Exception as e:
        print(f"âš ï¸  Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ HTML {file_path.name}: {e}")
    
    return None

def process_pdf_file(file_path):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ PDF - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ùˆ Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ØªÙˆØ§"""
    filename = file_path.name
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„
    title = extract_title_from_filename(filename)
    if not title:
        # Ø§Ú¯Ø± Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø¹Ø¯Ø¯ÛŒ Ø§Ø³ØªØŒ Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        title = filename.replace('.pdf', '').replace('_', ' ')
    
    # ØªØ¹ÛŒÛŒÙ† Ù†ÙˆØ¹ Ù…Ø­ØªÙˆØ§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù… ÙØ§ÛŒÙ„
    content_type = "news"
    category = "Ø¹Ù…ÙˆÙ…ÛŒ"
    tags = "Ø³Ù†Ø¯,PDF"
    
    if "Ø³Ù…ÛŒÙ†Ø§Ø±" in filename or "Ù‡Ù…Ø§ÛŒØ´" in filename or "Ú©Ù†Ú¯Ø±Ù‡" in filename:
        content_type = "event"
        category = "Ù‡Ù…Ø§ÛŒØ´"
        tags = "Ù‡Ù…Ø§ÛŒØ´,Ø³Ù…ÛŒÙ†Ø§Ø±,Ú©Ù†Ú¯Ø±Ù‡"
    elif "ØªØ§Ø±ÛŒØ®Ú†Ù‡" in filename or "history" in filename.lower():
        category = "ØªØ§Ø±ÛŒØ®Ú†Ù‡"
        tags = "ØªØ§Ø±ÛŒØ®Ú†Ù‡,Ø³Ù†Ø¯"
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ØªÙˆØ§ÛŒ ØªÙˆØµÛŒÙÛŒ
    content = f"Ø§ÛŒÙ† ÙØ§ÛŒÙ„ PDF Ø¨Ø§ Ù†Ø§Ù… '{filename}' Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ù…Ø­ØªÙˆØ§ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯. Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§Ù…Ù„ØŒ Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ PDF Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯."
    
    return {
        "type": content_type,
        "title": title,
        "slug": persian_to_slug(title),
        "content": content,
        "short_content": extract_short_content(content),
        "category": category,
        "tags": tags,
        "source": "Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§ÛŒØ±Ø§Ù†",
        "file_path": f"Content/{filename}",
        "image": None
    }

def process_docx_file(file_path):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Word"""
    filename = file_path.name
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„
    title = extract_title_from_filename(filename)
    if not title:
        title = filename.replace('.docx', '').replace('_', ' ')
    
    # ØªØ¹ÛŒÛŒÙ† Ù†ÙˆØ¹ Ù…Ø­ØªÙˆØ§
    content_type = "news"
    category = "Ø¹Ù…ÙˆÙ…ÛŒ"
    tags = "Ø³Ù†Ø¯,Word"
    
    if "Ø§Ø·Ù„Ø§Ø¹Ø§Øª" in filename or "404" in filename:
        category = "Ø§Ø·Ù„Ø§Ø¹Ø§Øª"
        tags = "Ø§Ø·Ù„Ø§Ø¹Ø§Øª,Ø³Ù†Ø¯"
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ØªÙˆØ§ÛŒ ØªÙˆØµÛŒÙÛŒ
    content = f"Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Word Ø¨Ø§ Ù†Ø§Ù… '{filename}' Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ù…Ø­ØªÙˆØ§ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯. Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§Ù…Ù„ØŒ Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ Word Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯."
    
    return {
        "type": content_type,
        "title": title,
        "slug": persian_to_slug(title),
        "content": content,
        "short_content": extract_short_content(content),
        "category": category,
        "tags": tags,
        "source": "Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§ÛŒØ±Ø§Ù†",
        "file_path": f"Content/{filename}",
        "image": None
    }

def create_news_from_image(file_path):
    """Ø§ÛŒØ¬Ø§Ø¯ Ø®Ø¨Ø± Ø§Ø² ØªØµÙˆÛŒØ±"""
    filename = file_path.name
    file_stem = Path(filename).stem
    
    # ØªØ¹ÛŒÛŒÙ† Ù†ÙˆØ¹ Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù… ÙØ§ÛŒÙ„
    category = "Ø¹Ù…ÙˆÙ…ÛŒ"
    tags = "ØªØµÙˆÛŒØ±,Ú¯Ø§Ù„Ø±ÛŒ"
    content_type = "news"
    
    if "photo_2025" in filename or "photo-2025" in filename:
        category = "Ù‡Ù…Ø§ÛŒØ´"
        tags = "Ù‡Ù…Ø§ÛŒØ´,ØªØµÙˆÛŒØ±,Ú¯Ø§Ù„Ø±ÛŒ,Ø±ÙˆÛŒØ¯Ø§Ø¯"
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÛŒØ® Ùˆ Ø²Ù…Ø§Ù† Ú©Ø§Ù…Ù„ Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„
        date_match = re.search(r'(\d{4})-(\d{2})-(\d{2})_(\d{2})-(\d{2})-(\d{2})', filename)
        if date_match:
            year, month, day, hour, minute, second = date_match.groups()
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø²Ù…Ø§Ù† Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ú©Ø±Ø¯Ù†
            title = f"ØªØµÙˆÛŒØ± Ù‡Ù…Ø§ÛŒØ´ Ø³Ø§Ù„ÛŒØ§Ù†Ù‡ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† - {year}/{month}/{day} Ø³Ø§Ø¹Øª {hour}:{minute}:{second}"
        else:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù…Ø§Ø±Ù‡ Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„
            number_match = re.search(r'(\d{2})$', file_stem)
            if number_match:
                img_num = number_match.group(1)
                title = f"ØªØµÙˆÛŒØ± Ù‡Ù…Ø§ÛŒØ´ Ø³Ø§Ù„ÛŒØ§Ù†Ù‡ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† - Ø´Ù…Ø§Ø±Ù‡ {img_num}"
            else:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¨Ø®Ø´ Ø¢Ø®Ø± Ù†Ø§Ù… ÙØ§ÛŒÙ„
                file_parts = file_stem.split('_')
                if len(file_parts) > 1:
                    img_id = file_parts[-1]
                    title = f"ØªØµÙˆÛŒØ± Ù‡Ù…Ø§ÛŒØ´ Ø³Ø§Ù„ÛŒØ§Ù†Ù‡ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† - {img_id}"
                else:
                    title = f"ØªØµÙˆÛŒØ± Ù‡Ù…Ø§ÛŒØ´ Ø³Ø§Ù„ÛŒØ§Ù†Ù‡ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† - {file_stem}"
        
        content = f"Ø§ÛŒÙ† ØªØµÙˆÛŒØ± Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ ØªØµØ§ÙˆÛŒØ± Ù‡Ù…Ø§ÛŒØ´ Ø³Ø§Ù„ÛŒØ§Ù†Ù‡ Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§ÛŒØ±Ø§Ù† Ø§Ø³Øª. ØªØµÙˆÛŒØ± Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ø¹Ù„Ù…ÛŒ Ùˆ ØªØ®ØµØµÛŒ Ø¨Ø±Ú¯Ø²Ø§Ø± Ø´Ø¯Ù‡ Ø¯Ø± Ø²Ù…ÛŒÙ†Ù‡ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø±ÛŒÙˆÛŒ Ú©ÙˆØ¯Ú©Ø§Ù† Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯."
        
    elif "img_2025" in filename or "img-2025" in filename:
        category = "Ø®Ø¨Ø±"
        tags = "Ø®Ø¨Ø±,ØªØµÙˆÛŒØ±"
        date_match = re.search(r'(\d{4})(\d{2})(\d{2})', filename)
        if date_match:
            year, month, day = date_match.groups()
            title = f"ØªØµÙˆÛŒØ± Ø®Ø¨Ø± - {year}/{month}/{day}"
        else:
            title = "ØªØµÙˆÛŒØ± Ø®Ø¨Ø±"
        
        content = f"Ø§ÛŒÙ† ØªØµÙˆÛŒØ± Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§Ø®Ø¨Ø§Ø± Ùˆ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§ÛŒØ±Ø§Ù† Ø§Ø³Øª."
        
    elif "image" in filename.lower():
        category = "Ø¹Ù…ÙˆÙ…ÛŒ"
        tags = "ØªØµÙˆÛŒØ±,Ú¯Ø§Ù„Ø±ÛŒ"
        title = "ØªØµÙˆÛŒØ± Ú¯Ø§Ù„Ø±ÛŒ Ø§Ù†Ø¬Ù…Ù†"
        content = f"Ø§ÛŒÙ† ØªØµÙˆÛŒØ± Ø§Ø² Ú¯Ø§Ù„Ø±ÛŒ ØªØµØ§ÙˆÛŒØ± Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§ÛŒØ±Ø§Ù† Ø§Ø³Øª."
        
    else:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ø§Ø² Ù†Ø§Ù… ÙØ§ÛŒÙ„
        title = file_stem.replace('_', ' ').replace('-', ' ')
        # Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ ØªØ§Ø±ÛŒØ®
        title = re.sub(r'\d{4}[-_]?\d{2}[-_]?\d{2}', '', title)
        title = re.sub(r'\(\d+\)', '', title)
        title = title.strip()
        
        if not title or len(title) < 3:
            title = f"ØªØµÙˆÛŒØ± {file_stem}"
        
        content = f"Ø§ÛŒÙ† ØªØµÙˆÛŒØ± Ø¨Ø§ Ù†Ø§Ù… '{filename}' Ø¯Ø± Ú¯Ø§Ù„Ø±ÛŒ ØªØµØ§ÙˆÛŒØ± Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§ÛŒØ±Ø§Ù† Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯."
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§Ù…Ù„
    full_content = f"{content}\n\nØ§ÛŒÙ† ØªØµÙˆÛŒØ± Ø¨Ø®Ø´ÛŒ Ø§Ø² Ø¢Ø±Ø´ÛŒÙˆ ØªØµØ§ÙˆÛŒØ± Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§ÛŒØ±Ø§Ù† Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ Ùˆ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ Ùˆ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ø¹Ù„Ù…ÛŒ Ø§ÛŒÙ† Ø§Ù†Ø¬Ù…Ù† Ø§Ø³Øª."
    
    slug = persian_to_slug(title)
    if not slug or len(slug) < 5:
        slug = persian_to_slug(file_stem) or f"image-{abs(hash(filename)) % 10000}"
    
    return {
        "type": "news",
        "title": title,
        "slug": slug,
        "content": full_content,
        "short_content": extract_short_content(full_content),
        "category": category,
        "tags": tags,
        "source": "Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§ÛŒØ±Ø§Ù†",
        "image": f"Content/{filename}",
        "is_published": True,
        "author": 1,
        "views": 0,
        "file_path": None
    }

def process_other_txt():
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ other.txt - ØªÙÚ©ÛŒÚ© Ø¨Ù‡ Ú†Ù†Ø¯ Ù…Ø­ØªÙˆØ§"""
    news_items = []
    
    try:
        with open(CONTENT_DIR / "other.txt", "r", encoding="utf-8") as f:
            content = f.read()
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ
        sections = re.split(r'\d+\.\s*Ù„ÛŒÙ†Ú©', content)
        
        for i, section in enumerate(sections[1:], 1):
            lines = section.strip().split('\n')
            if not lines:
                continue
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù†
            title = None
            date = None
            source = None
            content_text = []
            
            for line in lines:
                line = line.strip()
                if line.startswith('Ø¹Ù†ÙˆØ§Ù†:'):
                    title = line.replace('Ø¹Ù†ÙˆØ§Ù†:', '').strip()
                elif line.startswith('ØªØ§Ø±ÛŒØ® Ø§Ù†ØªØ´Ø§Ø±:'):
                    date_str = line.replace('ØªØ§Ø±ÛŒØ® Ø§Ù†ØªØ´Ø§Ø±:', '').strip()
                    date = date_str
                elif line.startswith('Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§Ù…Ù„'):
                    continue
                elif line and not line.startswith('http'):
                    content_text.append(line)
            
            if title and content_text:
                full_content = '\n\n'.join(content_text)
                # Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ø² Ù…Ø­ØªÙˆØ§
                full_content = re.sub(r'https?://[^\s]+', '', full_content)
                full_content = re.sub(r'\s+', ' ', full_content).strip()
                
                slug = persian_to_slug(title)
                if not slug or len(slug) < 5:
                    slug = f"khabar-{i}"
                
                # ØªØ¹ÛŒÛŒÙ† Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØªÙˆØ§
                category = "Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÙ†ÙØ³ÛŒ"
                tags = "Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù†,Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÙ†ÙØ³ÛŒ"
                
                if "Ø¯Ø®Ø§Ù†ÛŒØ§Øª" in full_content or "Ù‚Ù„ÛŒØ§Ù†" in full_content:
                    tags += ",Ø¯Ø®Ø§Ù†ÛŒØ§Øª,Ù‡Ø´Ø¯Ø§Ø±"
                if "Ø¢Ù„ÙˆØ¯Ú¯ÛŒ" in full_content:
                    tags += ",Ø¢Ù„ÙˆØ¯Ú¯ÛŒ Ù‡ÙˆØ§"
                if "Ú©Ù†Ú¯Ø±Ù‡" in full_content or "Ù‡Ù…Ø§ÛŒØ´" in full_content:
                    category = "Ù‡Ù…Ø§ÛŒØ´"
                    tags += ",Ù‡Ù…Ø§ÛŒØ´,Ú©Ù†Ú¯Ø±Ù‡"
                
                news_item = {
                    "type": "news",
                    "title": title,
                    "slug": slug,
                    "content": full_content,
                    "short_content": extract_short_content(full_content),
                    "category": category,
                    "tags": tags,
                    "source": "Ø´ÙÙ‚Ù†Ø§ / Ù…Ù‡Ø±Ù†ÛŒÙˆØ² / ÙˆØ¨Ø¯Ø§",
                    "image": None
                }
                news_items.append(news_item)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù‡Ù…Ø§ÛŒØ´ Ø§Ø² other.txt
        agenda_match = re.search(r'Ù¾Ù†Ø¬Ù…ÛŒÙ† Ù‡Ù…Ø§ÛŒØ´ Ø³Ø§Ù„ÛŒØ§Ù†Ù‡.*?Ù¾Ø§ÛŒØ§Ù† Ù‡Ù…Ø§ÛŒØ´', content, re.DOTALL)
        if agenda_match:
            agenda_text = agenda_match.group(0)
            agenda_text = re.sub(r'https?://[^\s]+', '', agenda_text)
            agenda_text = re.sub(r'\s+', ' ', agenda_text).strip()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒØ³Øª Ø³Ø®Ù†Ø±Ø§Ù†Ø§Ù†
            speakers_list = []
            speaker_pattern = r'Ø¯Ú©ØªØ±\s+[^\n]+'
            speakers = re.findall(speaker_pattern, agenda_text)
            unique_speakers = list(set(speakers))[:20]
            speakers_text = 'ØŒ '.join(unique_speakers[:10]) if unique_speakers else "Ø¯Ú©ØªØ± Ù‚Ù…Ø±ØªØ§Ø¬ Ø®Ø§Ù†Ø¨Ø§Ø¨Ø§ÛŒÛŒ Ùˆ Ø³Ø§ÛŒØ± Ø§Ø³Ø§ØªÛŒØ¯"
            
            title = "Ù¾Ù†Ø¬Ù…ÛŒÙ† Ù‡Ù…Ø§ÛŒØ´ Ø³Ø§Ù„ÛŒØ§Ù†Ù‡ Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§ÛŒØ±Ø§Ù†"
            slug = persian_to_slug(title)
            
            event = {
                "type": "event",
                "title": title,
                "slug": slug,
                "description": agenda_text,
                "short_description": "Ù¾Ù†Ø¬Ù…ÛŒÙ† Ù‡Ù…Ø§ÛŒØ´ Ø³Ø§Ù„ÛŒØ§Ù†Ù‡ Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§ÛŒØ±Ø§Ù† Ø¯Ø± ØªØ§Ø±ÛŒØ® Û³Û± Ø®Ø±Ø¯Ø§Ø¯ Ùˆ Û± ØªÛŒØ± Û±Û´Û°Û³ Ø¯Ø± Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù† Ú©ÙˆØ¯Ú©Ø§Ù† Ù…ÙÛŒØ¯ ØªÙ‡Ø±Ø§Ù† Ø¨Ø±Ú¯Ø²Ø§Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯.",
                "event_type": "seminar",
                "location": "ØªÙ‡Ø±Ø§Ù† - Ø®ÛŒØ§Ø¨Ø§Ù† Ø´Ø±ÛŒØ¹ØªÛŒØŒ Ù†Ø±Ø³ÛŒØ¯Ù‡ Ø¨Ù‡ Ù…ÛŒØ±Ø¯Ø§Ù…Ø§Ø¯ØŒ Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù† Ú©ÙˆØ¯Ú©Ø§Ù† Ù…ÙÛŒØ¯",
                "event_month": 3,
                "event_year": 1403,
                "agenda": agenda_text,
                "organizer": "Ø§Ù†Ø¬Ù…Ù† Ø¹Ù„Ù…ÛŒ Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ø§ÛŒØ±Ø§Ù†",
                "target_audience": "ÙÙˆÙ‚â€ŒØªØ®ØµØµØ§Ù† Ø±ÛŒÙ‡ØŒ Ù…ØªØ®ØµØµØ§Ù† Ú©ÙˆØ¯Ú©Ø§Ù† Ùˆ Ù¾Ø²Ø´Ú©Ø§Ù† Ø¹Ù…ÙˆÙ…ÛŒ",
                "speakers": speakers_text,
                "contact_info": "ØªÙ„ÙÙ† ÙˆØ§Ø­Ø¯ Ù…Ø¬Ø±ÛŒ: Û°Û²Û±-Û¸Û¸Û¸Û·Û´Û¸Û¸Ûµ | Ú©Ø¯ Ø´Ù†Ø§Ø³Ù‡: Û²Û±Û²Û³Û¶Û´",
                "file_path": None,
                "cover_image": None
            }
            news_items.append(event)
    
    except Exception as e:
        print(f"âš ï¸  Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ other.txt: {e}")
    
    return news_items

def validate_and_clean_data(data_list, data_type):
    """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
    cleaned = []
    seen_slugs = set()
    
    for item in data_list:
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø§Ø¬Ø¨Ø§Ø±ÛŒ
        content_field = item.get("content") or item.get("description")
        if not item.get("title") or not item.get("slug") or not content_field:
            print(f"âš ï¸  Ù‡Ø´Ø¯Ø§Ø±: {data_type} Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù† '{item.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')}' ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø§Ø¬Ø¨Ø§Ø±ÛŒ Ù†Ø¯Ø§Ø±Ø¯ Ùˆ Ø­Ø°Ù Ø´Ø¯.")
            continue
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÛŒÚ©ØªØ§ÛŒÛŒ Ø§Ø³Ù„Ø§Ú¯
        slug = item["slug"]
        if slug in seen_slugs:
            counter = 1
            while f"{slug}-{counter}" in seen_slugs:
                counter += 1
            slug = f"{slug}-{counter}"
            item["slug"] = slug
        seen_slugs.add(slug)
        
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§
        if item.get("content"):
            item["content"] = re.sub(r'\s+', ' ', item["content"]).strip()
        if item.get("description"):
            item["description"] = re.sub(r'\s+', ' ', item["description"]).strip()
        
        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ Ø¹Ù†ÙˆØ§Ù†
        if len(item["title"]) > 200:
            item["title"] = item["title"][:197] + "..."
        
        cleaned.append(item)
    
    return cleaned

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ - Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ù…Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§"""
    print("Ø´Ø±ÙˆØ¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø­ØªÙˆØ§...")
    print("=" * 60)
    
    result = {
        "news": [],
        "announcements": [],
        "events": [],
        "metadata": {
            "extracted_at": datetime.now().isoformat(),
            "total_news": 0,
            "total_announcements": 0,
            "total_events": 0,
            "version": "3.0"
        }
    }
    
    # Ù„ÛŒØ³Øª Ù‡Ù…Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
    all_files = list(CONTENT_DIR.iterdir())
    
    print(f"ğŸ“ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§: {len(all_files)}")
    print()
    
    # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø± ÙØ§ÛŒÙ„
    for file_path in all_files:
        if file_path.is_file():
            filename = file_path.name.lower()
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ HTML
            if filename.endswith('.html'):
                print(f"ğŸ“„ Ù¾Ø±Ø¯Ø§Ø²Ø´ HTML: {file_path.name}")
                item = process_html_file(file_path)
                if item:
                    if item["type"] == "news":
                        result["news"].append(item)
                    elif item["type"] == "event":
                        result["events"].append(item)
                print(f"   âœ“ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ PDF
            elif filename.endswith('.pdf'):
                print(f"ğŸ“„ Ù¾Ø±Ø¯Ø§Ø²Ø´ PDF: {file_path.name}")
                item = process_pdf_file(file_path)
                if item:
                    if item["type"] == "news":
                        result["news"].append(item)
                    elif item["type"] == "event":
                        result["events"].append(item)
                print(f"   âœ“ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Word
            elif filename.endswith('.docx'):
                print(f"ğŸ“„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Word: {file_path.name}")
                item = process_docx_file(file_path)
                if item:
                    if item["type"] == "news":
                        result["news"].append(item)
                print(f"   âœ“ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ ØªØµÙˆÛŒØ± - ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø®Ø¨Ø±
            elif filename.endswith(('.jpg', '.jpeg', '.png', '.gif')):
                print(f"ğŸ–¼ï¸  Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ± Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø®Ø¨Ø±: {file_path.name}")
                news_item = create_news_from_image(file_path)
                result["news"].append(news_item)
                print(f"   âœ“ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø®Ø¨Ø± Ø´Ø¯: {news_item['title']}")
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ other.txt (Ú†Ù†Ø¯ Ù…Ø­ØªÙˆØ§)
            elif filename == 'other.txt':
                print(f"ğŸ“„ Ù¾Ø±Ø¯Ø§Ø²Ø´ other.txt (Ú†Ù†Ø¯ Ù…Ø­ØªÙˆØ§):")
                items = process_other_txt()
                for item in items:
                    if item["type"] == "news":
                        result["news"].append(item)
                    elif item["type"] == "event":
                        result["events"].append(item)
                print(f"   âœ“ {len(items)} Ù…Ø­ØªÙˆØ§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")
    
    # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
    print("\nğŸ” Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
    result["news"] = validate_and_clean_data(result["news"], "Ø®Ø¨Ø±")
    result["events"] = validate_and_clean_data(result["events"], "Ø±ÙˆÛŒØ¯Ø§Ø¯")
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø§Ø¬Ø¨Ø§Ø±ÛŒ
    for item in result["news"]:
        item["is_published"] = True
        item["author"] = 1
        item["views"] = 0
        if "file_path" not in item:
            item["file_path"] = None
    
    for item in result["events"]:
        item["is_published"] = True
        item["is_featured"] = True
        item["created_by"] = 1
        item["price"] = 0
        item["views"] = 0
        if "cover_image" not in item:
            item["cover_image"] = None
    
    # Ù…Ø±ØªØ¨Ø· Ú©Ø±Ø¯Ù† ØªØµØ§ÙˆÛŒØ± Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø¨Ø§ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ (Ø¨Ø±Ø§ÛŒ cover_image)
    print("\nğŸ”— Ù…Ø±ØªØ¨Ø· Ú©Ø±Ø¯Ù† ØªØµØ§ÙˆÛŒØ± Ø¨Ø§ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§...")
    event_news = [n for n in result["news"] if n.get("category") == "Ù‡Ù…Ø§ÛŒØ´" and n.get("image")]
    
    if result["events"] and event_news:
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÙˆÙ„ÛŒÙ† ØªØµÙˆÛŒØ± Ù‡Ù…Ø§ÛŒØ´ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ú©Ø§ÙˆØ±
        result["events"][0]["cover_image"] = event_news[0]["image"]
        print(f"   âœ“ ØªØµÙˆÛŒØ± Ú©Ø§ÙˆØ± Ø¨Ù‡ Ø±ÙˆÛŒØ¯Ø§Ø¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø·Ù„Ø§Ø¹ÛŒÙ‡â€ŒÙ‡Ø§
    print("\nğŸ“¢ Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø·Ù„Ø§Ø¹ÛŒÙ‡â€ŒÙ‡Ø§...")
    announcements = [
        {
            "title": "Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ú©Ù†Ú¯Ø±Ù‡ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø±ÛŒÙˆÛŒ Ú©ÙˆØ¯Ú©Ø§Ù† Û±Û´Û°Û³",
            "slug": "bargozari-kongreh-bimarihaye-riuye-kudakan-1403",
            "content": "Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹ Ù…ÛŒâ€ŒØ±Ø³Ø§Ù†Ø¯ Ú©Ù†Ú¯Ø±Ù‡ Ø¨ÛŒÙ…Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø±ÛŒÙˆÛŒ Ú©ÙˆØ¯Ú©Ø§Ù† Û±Û´Û°Û³ Ø¯Ø± ØªØ§Ø±ÛŒØ® Û³Û± Ø®Ø±Ø¯Ø§Ø¯ Ù„ØºØ§ÛŒØª Û² ØªÛŒØ± Ù…Ø§Ù‡ Û±Û´Û°Û³ Ø¯Ø± Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù† Ú©ÙˆØ¯Ú©Ø§Ù† Ù…ÙÛŒØ¯ ØªÙ‡Ø±Ø§Ù† Ø¨Ø±Ú¯Ø²Ø§Ø± Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.",
            "is_published": True,
            "is_important": True,
            "author": 1,
            "image": None,
            "views": 0
        },
        {
            "title": "Ù‡Ø´Ø¯Ø§Ø±: Ù…ØµØ±Ù Ø¯Ø®Ø§Ù†ÛŒØ§Øª Ø¯Ø± Ù…ÛŒØ§Ù† Ù†ÙˆØ¬ÙˆØ§Ù†Ø§Ù† Ù†Ú¯Ø±Ø§Ù†â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø§Ø³Øª",
            "slug": "hoshdar-masraf-dokhaniyat-dar-mian-nojavanan",
            "content": "Ø±Ø¦ÛŒØ³ Ø§Ù†Ø¬Ù…Ù† Ø±ÛŒÙ‡ Ú©ÙˆØ¯Ú©Ø§Ù† Ù‡Ø´Ø¯Ø§Ø± Ø¯Ø§Ø¯: Ù…ØµØ±Ù Ø¯Ø®Ø§Ù†ÛŒØ§Øª Ø¯Ø± Ù…ÛŒØ§Ù† Ù†ÙˆØ¬ÙˆØ§Ù†Ø§Ù† Ù†Ú¯Ø±Ø§Ù†â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø§Ø³Øª.",
            "is_published": True,
            "is_important": True,
            "author": 1,
            "image": None,
            "views": 0
        }
    ]
    result["announcements"] = validate_and_clean_data(announcements, "Ø§Ø·Ù„Ø§Ø¹ÛŒÙ‡")
    
    # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
    result["metadata"]["total_news"] = len(result["news"])
    result["metadata"]["total_announcements"] = len(result["announcements"])
    result["metadata"]["total_events"] = len(result["events"])
    
    # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ JSON
    print(f"\nğŸ’¾ Ø¯Ø± Ø­Ø§Ù„ Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ JSON...")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print("=" * 60)
    print(f"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")
    print(f"\nğŸ“Š Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ:")
    print(f"   ğŸ“° Ø§Ø®Ø¨Ø§Ø±: {result['metadata']['total_news']}")
    print(f"   ğŸ“¢ Ø§Ø·Ù„Ø§Ø¹ÛŒÙ‡â€ŒÙ‡Ø§: {result['metadata']['total_announcements']}")
    print(f"   ğŸ“… Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§: {result['metadata']['total_events']}")
    print(f"\nğŸ“ ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ: {OUTPUT_FILE}")
    print("=" * 60)

if __name__ == "__main__":
    main()
